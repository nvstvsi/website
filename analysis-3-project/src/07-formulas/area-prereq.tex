\section{Linear Algebra Prerequisites}

Here we collect some results which will be used in our discussion and proof of the area formula.
All of this could be done in a greater generality, but we will restrict ourselves to linear maps between Euclidean spaces for simplicity.

\subsection{Review}

\begin{definition}[Orthogonal Map, Symmetric Map, Adjoint Map]
    \label{def:orthogonal_matrix}
    A linear map $R : \R^n \to \R^m$ is called \textbf{orthogonal} if it preserves the inner product, i.e.
    \[ \langle R(x), R(y) \rangle = \langle x, y \rangle \quad \text{ for all } x, y \in \R^n. \]
    The set of all orthogonal linear maps $\R^n \to \R^m$ is denoted $O(n,m)$.
    We also set $O(n) := O(n,n)$.

    \vspace{2mm}

    \noindent A linear map $S : \R^n \to \R^n$ is called \textbf{symmetric} if
    \[ \langle S(x), y \rangle = \langle x, S(y) \rangle \quad \text{ for all } x, y \in \R^n. \]

    \vspace{2mm}

    \noindent Given a linear map $T : \R^n \to \R^m$, the \textbf{adjoint} of $T$ is the unique linear map $T^* : \R^m \to \R^n$ such that
    \[ \langle T(x), y \rangle = \langle x, T^*(y) \rangle \quad \text{ for all } x \in \R^n, y \in \R^m. \]
\end{definition}

\begin{exercise}[Linear Algebra Review]
    \label{ex:orthogonal_properties}
    \begin{enumerate}[(i)]
        \item $A^{**} = A$ for each linear map $A : \R^n \to \R^m$.
        \item $(AB)^* = B^* A^*$ for each pair of linear maps $A : \R^n \to \R^m$ and $B : \R^m \to \R^k$.
        \item $R \in O(n,m)$ if and only if $R^* R = \operatorname{id}_{\R^n}$.
        \item If $R \in O(n,m)$ then $n \leq m$ and $R R^*(y) = y$ for all $y \in \operatorname{im}(R)$.
        \item $R \in O(n)$ if and only if $R^{-1} = R^*$.
        \item A linear map $S : \R^n \to \R^n$ is symmetric if and only if $S = S^*$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[(i)]
        \item Let $A: \R^n \to \R^m$ be a linear map. Then for each $x \in \R^n$ and $y \in \R^m$, we have
            \begin{align*}
                \langle A^{**}(x), y \rangle &= \langle x, A^*(y) \rangle \\
                    &= \langle A(x), y \rangle.
            \end{align*}
            Since this holds for all $x$ and $y$, we have $A^{**} = A$.

        \item Let $A : \R^n \to \R^m$ and $B : \R^m \to \R^k$ be linear maps. Then for each $x \in \R^n$ and $y \in \R^k$, we have
            \begin{align*}
                \langle (AB)^*(y), x \rangle &= \langle y, AB(x) \rangle \\
                    &= \langle B^*(y), A(x) \rangle \\
                    &= \langle A^* B^*(y), x \rangle.
            \end{align*}
            Since this holds for all $x$ and $y$, we have $(AB)^* = B^* A^*$.

        \item Suppose $R \in O(n,m)$. Then for each $x, y \in \R^n$, we have
            \[ \langle R^* R(x), y \rangle = \langle R(x), R(y) \rangle = \langle x, y \rangle. \]
            Since this holds for all $x$ and $y$, we have $R^* R = \operatorname{id}_{\R^n}$.

            Conversely, suppose $R^* R = \operatorname{id}_{\R^n}$. Then for each $x, y \in \R^n$, we have
            \[ \langle R(x), R(y) \rangle = \langle R^* R(x), y \rangle = \langle x, y \rangle. \]
            Since this holds for all $x$ and $y$, we see that $R$ is orthogonal.

        \item Suppose $R \in O(n,m)$. By exercise (iii), we have $R^* R = \operatorname{id}_{\R^n}$.
            Then note that $R^* : \R^m \to \R^n$ is surjective (since $R^* R$ is the identity on $\R^n$, and hence surjective).
            Thus, by the rank-nullity theorem, we have
            \[ m = \dim(\ker(R^*)) + \dim(\operatorname{im}(R^*)) = \dim(\ker(R^*)) + n, \]
            so $n \leq m$.

            Now let $y \in \operatorname{im}(R)$. Then there exists $x \in \R^n$ such that $y = R(x)$.
            Thus,
            \[ R R^*(y) = R R^* R(x) = R(x) = y \]
            as desired. 

        \item Suppose $R \in O(n)$. Then we have $R^* R = \operatorname{id}_{\R^n}$ by part (iii). Multiplying both sides on the left by $R^{-1}$, we have $R^{-1} = R^*$.

            Conversely, suppose $R^{-1} = R^*$. Then we have
            \[ R^* R = R^{-1} R = \operatorname{id}_{\R^n}. \]
            By part (iii), we see that $R$ is orthogonal.

        \item Suppose $S:\R^n \to \R^n$ is symmetric. Then for each $x, y \in \R^n$, we have
            \[ \langle S(x), y \rangle = \langle x, S(y) \rangle = \langle x, S^*(y) \rangle. \]
            Since this holds for all $x$ and $y$, we have $S = S^*$.

            Conversely, suppose $S = S^*$. Then for each $x, y \in \R^n$, we have
            \[ \langle S(x), y \rangle = \langle x, S^*(y) \rangle = \langle x, S(y) \rangle. \]
            Since this holds for all $x$ and $y$, we see that $S$ is symmetric.
    \end{enumerate}
\end{proof}

We will also recall the Spectral Decomposition Theorem for symmetric matrices, without proof.

\begin{theorem}[Spectral Decomposition Theorem]
    \label{thm:spectral_decomposition}
    For each symmetric linear map $S : \R^n \to \R^n$, there exists an orthogonal map $R \in O(n)$ and diagonal map $D : \R^n \to \R^n$ such that
    \[ S = R D R^{-1}. \]
    The diagonal entries of $D$ are the eigenvalues of $S$, and the columns of $R$ are the corresponding orthonormal eigenvectors of $S$.
\end{theorem}
\begin{remark}[Spectral Decomposition]
    \label{rem:spectral_decomposition}
    In other words, the Spectral Decomposition Theorem states that every symmetric linear map $S : \R^n \to \R^n$ has an orthonormal basis of eigenvectors $\{ v_1, \ldots, v_n \}$ with corresponding real eigenvalues $\lambda_1, \ldots, \lambda_n$ such that
    \[ S = \sum_{j=1}^n \lambda_j v_j\otimes v_j \]
    where $v_j \otimes v_j$ is the linear map defined by
    \[ (v_j \otimes v_j)(x) := \langle x, v_j \rangle v_j. \]
\end{remark}

We will use the Spectral Decomposition Theorem to prove the Polar Decomposition Theorem.
\begin{theorem}[Polar Decomposition Theorem]
    \label{thm:polar_decomposition}
    Let $T : \R^n \to \R^m$ be a linear map.
    \begin{enumerate}
        \item If $n \leq m$, then there exists an orthogonal map $R \in O(n,m)$ and a symmetric map $S : \R^n \to \R^n$ such that
            \[ T = R S. \]
        \item If $n \geq m$, then there exists an orthogonal map $R \in O(m,n)$ and a symmetric map $S : \R^m \to \R^m$ such that
            \[ T = S R^*. \]
    \end{enumerate}
\end{theorem}

You should think of this as being an abstraction of the polar form of complex numbers, where multiplication by a complex number is decomposed into a rotation (an orthogonal map) and a scaling (a symmetric map).

\begin{proof}
    Suppose $n \leq m$. Then note that $T^* T : \R^n \to \R^n$ is a symmetric linear map since 
    \[ (T^* T)^* = T^* (T^*)^* = T^* T \]
    by exercise \ref{ex:orthogonal_properties} (ii) and (i).
    Also note that $T^* T$ is positive semi-definite, since for each $x \in \R^n$, we have
    \[ \langle T^* T(x), x \rangle = \langle T(x), T(x) \rangle = \|T(x)\|^2 \geq 0. \]
    Hence there are non-negative eigenvalues $\lambda_1, \ldots, \lambda_n \geq 0$ of $T^* T$ with corresponding orthonormal eigenvectors $v_1, \ldots, v_n$ which form an orthonormal basis of $\R^n$ by the Spectral Decomposition Theorem \ref{thm:spectral_decomposition}.
    For each $j = 1, \ldots, n$, define
    \[ \mu_j := \sqrt{\lambda_j} \geq 0. \]
    If $\mu_j > 0$, define
    \[ w_j := \frac{1}{\mu_j} T(v_j) \in \R^m. \]
    If $\mu_j = 0$, then define $w_j \in \R^m$ to be any unit vector orthogonal to all of the previously defined $w_1, \ldots, w_{j-1}$.
    Then the set $\{w_1, \ldots, w_n\}$ is an orthogonal set in $\R^m$ because for each $i \neq j$, if $\mu_i, \mu_j > 0$, then
    \[ \langle w_i, w_j \rangle = \frac{1}{\mu_i \mu_j} \langle T(v_i), T(v_j) \rangle = \frac{1}{\mu_i \mu_j} \langle T^* T(v_i), v_j \rangle = \frac{\lambda_j}{\mu_i \mu_j} \langle v_i, v_j \rangle = 0, \]
    and if one of $\mu_i$ or $\mu_j$ is zero, then the orthogonality follows from the definition of $w_j$.

    Define the linear map $S : \R^n \to \R^n$ by
    \[ S(v_j) := \mu_j v_j \]
    and the linear map $R : \R^n \to \R^m$ by
    \[ R(v_j) := w_j. \]
    Then for each $j = 1, \ldots, n$, we have
    \[ R S(v_j) = \mu_j R(v_j) = \mu_j w_j = T(v_j). \]
    Thus, we have $T = R S$.
    Also, note that $S$ is symmetric since for each $i, j = 1, \ldots, n$, we have
    \[ \langle S(v_i), v_j \rangle = \mu_i \langle v_i, v_j \rangle = \mu_j \langle v_i, v_j \rangle = \langle v_i, S(v_j) \rangle. \]
    Finally, note that $R$ is orthogonal since for each $i, j = 1, \ldots, n$, we have
    \[ \langle R(v_i), R(v_j) \rangle = \langle w_i, w_j \rangle = \langle v_i, v_j \rangle. \]
    This completes the proof of part (i).

    \vspace{2mm}
    (ii). Suppose $n \geq m$. Then $T^*: \R^m \to \R^n$ is a linear map with $m \leq n$, so by part (i), there exists an orthogonal map $R \in O(m,n)$ and a symmetric map $S : \R^m \to \R^m$ such that
    \[ T^* = R S. \]
    Taking adjoints of both sides, we have
    \[ T = S R^*. \]
\end{proof}

\begin{definition}[Jacobian of a Linear Map]
    \label{def:jacobian}
    Let $T : \R^n \to \R^m$ be a linear map. 
    \begin{itemize}
        \item If $n \leq m$, then the \textit{Jacobian} of $T$ is
            \[ JT := |\det S| \]
            where $S : \R^n \to \R^n$ is the symmetric map from the polar decomposition $T = R S$ in theorem \ref{thm:polar_decomposition} (i).
        \item If $n \geq m$, then the \textit{Jacobian} of $T$ is
            \[ JT := |\det S| \]
            where $S : \R^m \to \R^m$ is the symmetric map from the polar decomposition $T = S R^*$ in theorem \ref{thm:polar_decomposition} (ii).
    \end{itemize}
\end{definition}

\begin{remark}[Well-Definedness of the Jacobian]
    \label{rem:well_defined_jacobian}
    At first, it may seem that the definition of the Jacobian in definition \ref{def:jacobian} depends on the choice of polar decomposition of $T$.
    We show that this is not the case.

    Suppose $n \leq m$, and let $T = R_1 S_1 = R_2 S_2$ be two polar decompositions of $T$ with $R_1, R_2 \in O(n,m)$ and symmetric maps $S_1, S_2 : \R^n \to \R^n$.
    Then we have
    \begin{align*}
        &R_1 S_1 = R_2 S_2 \\
        & \implies S_1 = R_1^{-1} R_2 S_2 \\
    \end{align*}
    so that
    \begin{align*}
        \det(S_1) &= \det(R_1^{-1} R_2 S_2) \\
            &= \det(R_1^{-1}) \det(R_2) \det(S_2) \\
            &= \pm1 \cdot \pm1 \cdot \det(S_2) \\
            &= \pm \det(S_2).
    \end{align*}
    since all orthogonal maps have determinant $\pm 1$.
    Thus, we have $|\det(S_1)| = |\det(S_2)|$ and as a result, the Jacobian $JT$ is well-defined.

    A similar argument holds in the case $n \geq m$.
\end{remark}

\begin{proposition}[Alternative Definition of the Jacobian]
    \label{prop:alt_definition_of_jacobian}
    Let $T : \R^n \to \R^m$ be a linear map. If $n \leq m$, then 
    \[ JT = \sqrt{\det(T^* T)} \]
    and if $n \geq m$, then
    \[ JT = \sqrt{\det(T T^*)}. \]
\end{proposition}

This is the definition we will use most often. 

\begin{proof}
    Suppose $n \leq m$. Let $T = R S$ be the polar decomposition of $T$ from theorem \ref{thm:polar_decomposition} (i) with $R \in O(n,m)$ and symmetric map $S : \R^n \to \R^n$.
    Then we have $T^* = S^* R^* = S R^*$ by exercise \ref{ex:orthogonal_properties} (i) and (vi), so that
    \[ T^* T = S R^* R S = S \circ \operatorname{id}_{\R^n} \circ S = S\circ S. \]
    As a result, we have
    \[ \det(T^* T) = \det(S\circ S) = \det(S)^2 \]
    so that
    \[ JT = |\det(S)| = \sqrt{\det(T^* T)}. \]

    \vspace{2mm}

    Now suppose $n \geq m$. Let $T = S R^*$ be the polar decomposition of $T$ from theorem \ref{thm:polar_decomposition} (ii) with $R \in O(m,n)$ and symmetric map $S : \R^m \to \R^m$.
    Then we have $T^* = R S$ by exercise \ref{ex:orthogonal_properties} (i) and (vi), so that
    \[ T T^* = S R^* R S = S \circ \operatorname{id}_{\R^m} \circ S = S\circ S. \]
    Hence
    \[ \det(T T^*) = \det(S\circ S) = \det(S)^2 \]
    so that 
    \[ JT = |\det(S)| = \sqrt{\det(T T^*)}. \]
\end{proof}

We conclude this section with a nice identity from Linear Algebra, which is a higher dimensional generalization of the Pythagorean theorem.

\begin{exercise}[Sum and Product Interchange]
    \label{ex:sum_product_interchange}
    Assume $n \leq m$. Let $(a_{j,k})_{1 \leq j \leq n, 1 \leq k \leq m}$ be a matrix of real numbers.
    Then
    \[ \prod_{j=1}^n \sum_{k=1}^m a_{j,k} = \sum_{(k_1,\ldots,k_n) \in \{1,\ldots,m\}^n} \prod_{j=1}^n a_{j, k_j} \]
\end{exercise}

\begin{proof}
    We prove this statement by induction on $n$. 

    \vspace{2mm}
    \textit{Base Case:} $n = 1$. Then on the left side of the desired equation, we have
    \[ \prod_{j=1}^1 \sum_{k=1}^m a_{j,k} = \sum_{k=1}^m a_{1,k} \]
    and on the right side of the desired equation, we have
    \[ \sum_{k \in\{1,\ldots,m\}} \prod_{j=1}^1 a_{j, k} = \sum_{k=1}^m a_{1,k} \]
    which is the same sum, so the base case holds.

    \vspace{2mm}
    \textit{Inductive Step:} Assume the statement holds for some $n \geq 1$.
    Then for $n+1$, we have
    \begin{align*}
        \prod_{j=1}^{n+1} \sum_{k=1}^m a_{j,k} &= \left( \prod_{j=1}^n \sum_{k=1}^m a_{j,k} \right) \left( \sum_{k=1}^m a_{n+1, k} \right) \\
            &= \left( \sum_{(k_1, \ldots, k_n) \in \{1, \ldots, m\}^n} \prod_{j=1}^n a_{j, k_j} \right) \left( \sum_{k=1}^m a_{n+1, k} \right) &&\text{by the inductive hypothesis} \\
            &= \sum_{(k_1, \ldots, k_n) \in \{1, \ldots, m\}^n} \sum_{k_{n+1}=1}^m \left( \prod_{j=1}^n a_{j, k_j} \right) a_{n+1, k_{n+1}} &&\text{by distributivity} \\
            &= \sum_{(k_1, \ldots, k_{n+1}) \in \{1, \ldots, m\}^{n+1}} \prod_{j=1}^{n+1} a_{j, k_j}.
    \end{align*}
    This completes the inductive step, and thus the proof.
\end{proof}

\begin{definition}
    \label{def:binet_cauchy_stuff}
    Assume $n \leq m$. Define 
    \[ \Lambda(n,m) := \{ \lambda : \{1, \ldots, n\} \to \{1, \ldots, m\} \mid \lambda \text{ is strictly increasing} \}. \]
    For each $\lambda \in \Lambda(n,m)$, define the linear map
    \[ P_\lambda : \R^m \to \R^n , \quad P_\lambda(x) := (x_{\lambda(1)}, \ldots, x_{\lambda(n)}) \]
    which is the projection onto the coordinates indexed by $\lambda$.
\end{definition}

\begin{proposition}[Binet-Cauchy Identity]
    \label{prop:binet_cauchy_identity}
    Assume $n \leq m$, and let $T : \R^n \to \R^m$ be a linear map.
    Then 
    \[ \det(T^* T) = \sum_{\sigma \in \Lambda(n,m)} \det(P_\sigma \circ T)^2. \]
\end{proposition}

In other words, when $n\leq m$ we can compute $\det(T^* T)$ by summing the squares of the determinants of all $n \times n$ minors of the standard matrix representation of $T$.

\begin{proof}
    Let $(t_{i,j})_{1 \leq i,j \leq n}$ be the standard matrix representation of $T : \R^n \to \R^m$, and let $(a_{i,j})_{1 \leq i,j \leq n}$ be the standard matrix representation of $T^* T : \R^n \to \R^n$.
    Then
    \[ a_{k,l} = \sum_{j=1}^n t_{j,k} t_{j,l}  \qquad \forall \, 1\leq k,l \leq n. \]
    By the alternative definition of the determinant from proposition \ref{prop:determinant_alternative_definition} and the definition of the determinant, we have
    \[ \det(T^* T) = \sum_{\sigma \in \operatorname{Sym}_n} \operatorname{sgn}(\sigma) \prod_{j=1}^n a_{j, \sigma(j)} \]
    where $\operatorname{Sym}_n$ is the group of bijections of $\{1, \ldots, n\}$, i.e. $\operatorname{Sym}_n$ is the symmetric group on $n$ elements.

    Continuing the above computation, using the expression for the entries $(a_{i,j})_{1 \leq i,j \leq n}$, we have
    \begin{align*}
        \det(T^*T) &= \sum_{\sigma \in \operatorname{Sym}_n} \operatorname{sgn}(\sigma) \prod_{j=1}^n \left( \sum_{k=1}^m t_{k,j} t_{k, \sigma(j)} \right) \\
            &= \sum_{\sigma \in \operatorname{Sym}_n} \operatorname{sgn}(\sigma) \sum_{(k_1, \ldots, k_n) \in\{1, \ldots, m\}^n} \prod_{j=1}^n t_{k_j, j} t_{k_j, \sigma(j)} \\
            &= \sum_{(k_1, \ldots, k_n) \in\{1, \ldots, m\}^n} \sum_{\sigma \in \operatorname{Sym}_n} \operatorname{sgn}(\sigma) \prod_{j=1}^n t_{k_j, j} t_{k_j, \sigma(j)}
    \end{align*}
    by exercise \ref{ex:sum_product_interchange}. Define the bijection
    \[ \{1, \ldots, m\}^{ \{1,\ldots, n\} } \longrightarrow \{1, \ldots, m\}^n, \qquad
        \phi \longmapsto (\phi(1), \ldots, \phi(n)) \]
    and see that if $\phi : \{1, \ldots, n\} \to \{1, \ldots, m\}$ is not injective, then there exist $i \neq j$ such that $\phi(i) = \phi(j)$ and thus the expression
    \[ \sum_{\sigma\in\operatorname{Sym}_n} \operatorname{sgn}(\sigma) \prod_{j=1}^n t_{\phi(j), j} t_{\phi(j), \sigma(j)} \]
    is the determinant of a matrix with two identical rows, and is thus zero.

    Therefore the expression for $\det(T^* T)$ above reduces to
    \[ \det(T^* T) = \sum_{\phi \in \operatorname{inj}(n,m)} \sum_{\sigma \in \operatorname{Sym}_n} \operatorname{sgn}(\sigma) \prod_{j=1}^n t_{\phi(j), j} t_{\phi(j), \sigma(j)} \]
    where $\operatorname{inj}(n,m)$ is the set of injective maps $\phi : \{1, \ldots, n\} \to \{1, \ldots, m\}$.

    Since each injection $\phi \in \operatorname{inj}(n,m)$ can be uniquely written as $\phi = \lambda \circ \vartheta$ for some $\lambda \in \Lambda(n,m)$ and $\vartheta \in \operatorname{Sym}_n$, we have
    \begin{align*}
        \det(T^*T) &= \sum_{\phi \in \operatorname{inj}(n,m)} \sum_{\sigma \in \operatorname{Sym}_n} \operatorname{sgn}(\sigma) \prod_{j=1}^n t_{\phi(j), j} t_{\phi(j), \sigma(j)} \\
            &= \sum_{\lambda \in \Lambda(n,m)} \sum_{\vartheta \in \operatorname{Sym}_n} \sum_{\sigma \in \operatorname{Sym}_n} \operatorname{sgn}(\sigma) \prod_{j=1}^n t_{\lambda(\vartheta(j)), j} t_{\lambda(\vartheta(j)), \sigma(j)} \\
            &= \sum_{\lambda \in \Lambda(n,m)} \sum_{\rho \in \operatorname{Sym}_n} \sum_{\vartheta \in \operatorname{Sym}_n} \operatorname{sgn}(\rho) \operatorname{sgn}(\vartheta) \prod_{j=1}^n t_{\lambda(\vartheta(j)), j} t_{\lambda(\vartheta(j)), \rho(\vartheta(j))} \\
            &= \sum_{\lambda \in \Lambda(n,m)} \left( \sum_{\vartheta \in \operatorname{Sym}_n} \operatorname{sgn}(\vartheta) \prod_{j=1}^n t_{\lambda(\vartheta(j)), j} \right) \left( \sum_{\rho \in \operatorname{Sym}_n} \operatorname{sgn}(\rho) \prod_{j=1}^n t_{\lambda(j), \rho(j)} \right) \\
            &= \sum_{\lambda \in \Lambda(n,m)} \left( \sum_{\vartheta \in \operatorname{Sym}_n} \operatorname{sgn}(\vartheta) \prod_{j=1}^n t_{\lambda(\vartheta(j)), j} \right)^2 \\
            &= \sum_{\lambda \in \Lambda(n,m)} \det(P_\lambda \circ T)^2
    \end{align*}
    as desired.
\end{proof}

\begin{theorem}[Area Formula for Linear Maps]
    \label{thm:area_formula_linear_maps}
    Assume $n \leq m$, and let $T : \R^n \to \R^m$ be a linear map.
    Then for each $A \subset \R^n$ we have
    \[ \H^n(T(A)) = JT \cdot \L^n(A). \]
\end{theorem}

This gives another interpretation of the Jacobian $JT$ of a linear map $T : \R^n \to \R^m$ when $n \leq m$: it is the factor by which $T$ scales $n$-dimensional Lebesgue measure.

\begin{proof}
    Use the Polar Decomposition Theorem \ref{thm:polar_decomposition} to write $T = R S$ with $R \in O(n,m)$ and symmetric map $S : \R^n \to \R^n$.
    Then be defintion $JT = |\det(S)|$.

    If $JT = 0$, then $S$ is not invertible, so $\dim(\operatorname{im}(S)) < n$ and thus $\dim(\operatorname{im}(T)) < n$.
    As a result $\H^n(T(A)) = 0$, so in this case both sides of the desired equation are zero.

    Thus we assume $JT > 0$, so that $S$ is invertible. Then for each $x\in \R^n$ and $r > 0$, we have
    \begin{align*}
        \frac{\H^n(T(B(x,r)))}{\L^n(B(x,r))} &= \frac{\H^n( R^* \circ T(B(x,r)))}{\L^n(B(x,r))} \\
            &=\frac{\L^n(R^*\circ T(B(x,r)))}{\L^n(B(x,r))} \\
            &= \frac{\L^n(R^* \circ R\circ S(B(x,r)))}{\L^n(B(x,r))} \\
            &= \frac{\L^n(S(B(x,r)))}{\L^n(B(x,r))} \\
            &= \frac{r^n \L^n(S(B(x,1)))}{r^n \omega_n} \\
            &= \frac{\L^n(S(B(x,1)))}{\omega_n} \\
            &= |\det(S)| = JT. 
    \end{align*}
    where we have used the fact that $R^*$ is an isometry $\operatorname{im}(T) \to \R^n$ in the first equality and that
    \[ \L^n(S(B(x,1))) = |\det(S)| \L^n(B(x,1)) = |\det(S)| \omega_n \]
    in the last equality.
    Therefore \[ \H^n(T(B(x,r))) = JT \cdot \L^n(B(x,r)) \tag{$\star$}\]
    for each $x \in \R^n$ and $r > 0$.

    Define 
    \[ \nu(A) := \H^n(T(A)) \qquad\forall \,A\subseteq \R^n. \]
    Then $\nu$ is a Borel regular outer measure on $\R^n$, and we claim that $\nu$ is absolutely continuous with respect to $\L^n$.

    If $A \subset \R^n$ is a set with $\L^n(A) = 0$, then for each $\epsilon > 0$, there exists a countable collection of open balls $\{ B(x_j, r_j) \}_{j=1}^\infty$ such that
    \[ A \subset \bigcup_{j=1}^\infty B(x_j, r_j) \quad \text{ and } \quad \sum_{j=1}^\infty \L^n(B(x_j, r_j)) < \epsilon. \]
    Thus, by countable subadditivity of $\nu$ and the calculation $(\star)$, we have
    \begin{align*}
        \nu(A) &\leq \sum_{j=1}^\infty \nu(B(x_j, r_j)) \\
            &= \sum_{j=1}^\infty \H^n(T(B(x_j, r_j))) \\
            &= \sum_{j=1}^\infty JT \cdot \L^n(B(x_j, r_j)) \\
            &< JT \cdot \epsilon.
    \end{align*}
    Since $\epsilon > 0$ was arbitrary, we have $\nu(A) = 0$, so $\nu$ is absolutely continuous with respect to $\L^n$.
    
    The Radon-Nikodym derivative of $\nu$ with respect to $\L^n$ is given by
    \[ D_{\L^n} \nu(x) = \lim_{r\to 0^+} \frac{\nu(B(x,r))}{\L^n(B(x,r))} = JT \]
    where the first equality holds by the Lebesgue-Radon-Nikodym Theorem \ref{thm:radon_nikodym_theorem_via_densities}, and the second equality follows from the above calculation $(\star)$.
    Thus for each Borel set $A \subset \R^n$, we have
    \[ \nu(A) = \int_A D_{\L^n} \nu(x) \,d\L^n(x) = \int_A JT \,d\L^n(x) = JT \cdot \L^n(A) \]
    by the Lebesgue-Radon-Nikodym Theorem \ref{thm:radon_nikodym_theorem_via_densities} again.
\end{proof}